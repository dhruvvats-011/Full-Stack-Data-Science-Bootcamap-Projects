{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8593ec56-8431-4969-91bb-0ea7d29d045c",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Explain One-Hot Encoding:\n",
    "\n",
    "One-Hot Encoding is a method of representing text data (or categorical data) in a numerical form. Each word (or category) is represented by a vector where only one element is 1, and all others are 0. The position of the 1 indicates the presence of the corresponding word in the vector space.\n",
    "Example: If we have three words: [\"cat\", \"dog\", \"fish\"], the one-hot encoded vectors will be:\n",
    "\"cat\" → [1, 0, 0]\n",
    "\"dog\" → [0, 1, 0]\n",
    "\"fish\" → [0, 0, 1]\n",
    "2. Explain Bag of Words:\n",
    "\n",
    "The Bag of Words (BoW) model is a simple and commonly used technique for text representation. It involves converting a collection of text (like a document) into a set of words (tokens) and representing it as a vector. The vector counts the occurrences of each word in the text. The order of the words is not preserved.\n",
    "Example: For the sentence \"cat and dog\", the BoW representation might look like:\n",
    "[\"cat\", \"dog\", \"and\"] → [1, 1, 1]\n",
    "This indicates each word appears once in the text.\n",
    "3. Explain Bag of N-Grams:\n",
    "\n",
    "The Bag of N-Grams is a modification of the BoW model. Instead of just counting individual words, it counts sequences of n consecutive words (called n-grams). This can capture some context by considering word combinations.\n",
    "Example: For the sentence \"cat and dog\", the 2-grams (bigram) representation might be:\n",
    "[\"cat and\", \"and dog\"] → [1, 1]\n",
    "4. Explain TF-IDF:\n",
    "\n",
    "TF-IDF (Term Frequency-Inverse Document Frequency) is a numerical statistic that reflects the importance of a word in a document relative to a collection (or corpus) of documents. It has two components:\n",
    "Term Frequency (TF): The number of times a word appears in a document.\n",
    "Inverse Document Frequency (IDF): A measure of how rare a word is across the entire corpus (higher IDF means the word is rare).\n",
    "The formula: TF-IDF = TF * IDF\n",
    "Words that appear frequently in a document but rarely across the corpus are considered more important.\n",
    "Example If \"cat\" appears frequently in one document but rarely across all documents, it gets a high TF-IDF score.\n",
    "5.What is the OOV problem?\n",
    "\n",
    "The Out-Of-Vocabulary (OOV) problem occurs when a word in the input text is not present in the vocabulary used by the model (e.g., not seen during training). This is common when dealing with rare or unseen words, which can affect the model's performance, especially for language models and word embeddings.\n",
    "Solutions to the OOV problem include techniques like using a special token (e.g., <UNK> for unknown words), subword tokenization (like Byte Pair Encoding), or character-level embeddings.\n",
    "6.What are word embeddings?\n",
    "\n",
    "Word embeddings are a type of word representation that allows words to be represented as dense vectors in a continuous vector space. Unlike one-hot encoding, which results in sparse vectors, word embeddings capture semantic relationships between words by positioning similar words closer in the vector space. They are learned from large corpora using models like Word2Vec, GloVe, or fastText.\n",
    "Example: The word embeddings for words like\"king\"and\"queen\" will be close in space and might reflect relationships like \"man\" to \"woman.\"\n",
    "\n",
    "7.Explain Continuous Bag of Words (CBOW)?\n",
    "Continuous Bag of Words (CBOW) is a model used in Word2Vec to learn word embeddings. In CBOW, the model predicts a target word (center word) based on the surrounding context words. The context words are averaged (or summed), and the resulting vector is used to predict the target word.\n",
    "Example: For the context \"the cat sits on the mat,\" the model predicts the word \"sits\" using surrounding words like \"the,\" \"cat,\" \"on,\" etc.\n",
    "8. Explain SkipGram:\n",
    "\n",
    "SkipGram is another Word2Vec model that works in the opposite way of CBOW. Given a target word, SkipGram tries to predict the context words (words around it). This model is particularly useful for handling rare words and works well for large corpora.\n",
    "Example: For the target word \"sits,\" the SkipGram model would predict the surrounding words like \"the,\" \"cat,\" \"on,\" etc.\n",
    "Explain GloVe Embeddings:\n",
    "\n",
    "GloVe (Global Vectors for Word Representation) is a word embedding model that generates dense word vectors by factorizing a word co-occurrence matrix. Unlike Word2Vec, which is a local context model, GloVe captures global statistical information from the entire corpus. It aims to find word vector representations that reflect the global corpus' statistical properties.\n",
    "Example: Words that often appear in similar contexts will have similar GloVe embeddings.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
